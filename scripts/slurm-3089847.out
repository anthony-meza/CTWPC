2023-05-10 20:22:58,053 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.16.3.126:45897'
2023-05-10 20:22:58,429 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ao12uf97', purging
2023-05-10 20:23:00,258 - distributed.worker - INFO -       Start worker at:   tcp://172.16.3.126:34350
2023-05-10 20:23:00,259 - distributed.worker - INFO -          Listening to:   tcp://172.16.3.126:34350
2023-05-10 20:23:00,259 - distributed.worker - INFO -           Worker name:             SLURMCluster-7
2023-05-10 20:23:00,259 - distributed.worker - INFO -          dashboard at:         172.16.3.126:44785
2023-05-10 20:23:00,259 - distributed.worker - INFO - Waiting to connect to:    tcp://172.16.3.56:38839
2023-05-10 20:23:00,259 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:23:00,259 - distributed.worker - INFO -               Threads:                         36
2023-05-10 20:23:00,259 - distributed.worker - INFO -                Memory:                  46.57 GiB
2023-05-10 20:23:00,259 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kvuhyfzu
2023-05-10 20:23:00,259 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:23:00,264 - distributed.worker - INFO -         Registered to:    tcp://172.16.3.56:38839
2023-05-10 20:23:00,264 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:23:00,264 - distributed.core - INFO - Starting established connection to tcp://172.16.3.56:38839
2023-05-10 20:23:02,163 - distributed.worker - WARNING - Compute Failed
Key:       _preprocess-013132cd-8b2a-41fa-9540-2cb49e658da0
Function:  _preprocess
args:      (<xarray.Dataset>
Dimensions:    (latitude: 745, longitude: 901)
Coordinates:
  * latitude   (latitude) float32 -2.0 -1.917 -1.833 -1.75 ... 59.83 59.92 60.0
  * longitude  (longitude) float32 -150.0 -149.9 -149.8 ... -75.17 -75.08 -75.0
Data variables:
    GC_E       (latitude, longitude) float32 dask.array<chunksize=(745, 901), meta=np.ndarray>
    GC_W       (latitude, longitude) float32 dask.array<chunksize=(745, 901), meta=np.ndarray>
    GC_C       (latitude, longitude) float32 dask.array<chunksize=(745, 901), meta=np.ndarray>
    SW         (latitude, longitude) float32 dask.array<chunksize=(745, 901), meta=np.ndarray>
    NW         (latitude, longitude) float32 dask.array<chunksize=(745, 901), meta=np.ndarray>
    EQ         (latitude, longitude) float32 dask.array<chunksize=(745, 901), meta=np.ndarray>
    COL        (latitude, longitude) float32 dask.array<chunksize=(745, 901), meta=np.ndarray>
Attributes: (12/13)
    title:                Bathymetry and mask for product GLO
kwargs:    {}
Exception: 'KeyError("\'depth\' is not a valid dimension or coordinate")'

/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in divide
  return func(*(_execute_task(a, cache) for a in args))
2023-05-10 20:25:36,845 - distributed.utils_perf - INFO - full garbage collection released 0.97 GiB from 801 reference cycles (threshold: 9.54 MiB)
2023-05-10 20:32:12,062 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 39.29 GiB -- Worker memory limit: 46.57 GiB
2023-05-10 20:32:12,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-10 20:32:16,019 - distributed.core - INFO - Removing comms to tcp://172.16.3.55:41968
2023-05-10 20:32:17,975 - distributed.nanny.memory - WARNING - Worker tcp://172.16.3.126:34350 (pid=140471) exceeded 95% memory budget. Restarting...
2023-05-10 20:32:21,637 - distributed.nanny - INFO - Worker process 140471 was killed by signal 15
2023-05-10 20:32:21,991 - distributed.nanny - WARNING - Restarting worker
2023-05-10 20:32:32,155 - distributed.worker - INFO -       Start worker at:   tcp://172.16.3.126:37246
2023-05-10 20:32:32,156 - distributed.worker - INFO -          Listening to:   tcp://172.16.3.126:37246
2023-05-10 20:32:32,156 - distributed.worker - INFO -           Worker name:             SLURMCluster-7
2023-05-10 20:32:32,156 - distributed.worker - INFO -          dashboard at:         172.16.3.126:39918
2023-05-10 20:32:32,156 - distributed.worker - INFO - Waiting to connect to:    tcp://172.16.3.56:38839
2023-05-10 20:32:32,156 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:32:32,156 - distributed.worker - INFO -               Threads:                         36
2023-05-10 20:32:32,156 - distributed.worker - INFO -                Memory:                  46.57 GiB
2023-05-10 20:32:32,156 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-19rkqwht
2023-05-10 20:32:32,157 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:32:32,164 - distributed.worker - INFO -         Registered to:    tcp://172.16.3.56:38839
2023-05-10 20:32:32,164 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:32:32,165 - distributed.core - INFO - Starting established connection to tcp://172.16.3.56:38839
/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in divide
  return func(*(_execute_task(a, cache) for a in args))
2023-05-10 20:35:10,543 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 38.36 GiB -- Worker memory limit: 46.57 GiB
2023-05-10 20:35:14,474 - distributed.nanny.memory - WARNING - Worker tcp://172.16.3.126:37246 (pid=141784) exceeded 95% memory budget. Restarting...
2023-05-10 20:35:18,060 - distributed.nanny - INFO - Worker process 141784 was killed by signal 15
2023-05-10 20:35:18,326 - distributed.nanny - WARNING - Restarting worker
2023-05-10 20:35:29,475 - distributed.worker - INFO -       Start worker at:   tcp://172.16.3.126:36031
2023-05-10 20:35:29,475 - distributed.worker - INFO -          Listening to:   tcp://172.16.3.126:36031
2023-05-10 20:35:29,475 - distributed.worker - INFO -           Worker name:             SLURMCluster-7
2023-05-10 20:35:29,475 - distributed.worker - INFO -          dashboard at:         172.16.3.126:33227
2023-05-10 20:35:29,475 - distributed.worker - INFO - Waiting to connect to:    tcp://172.16.3.56:38839
2023-05-10 20:35:29,476 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:35:29,476 - distributed.worker - INFO -               Threads:                         36
2023-05-10 20:35:29,476 - distributed.worker - INFO -                Memory:                  46.57 GiB
2023-05-10 20:35:29,476 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pqsvtm_f
2023-05-10 20:35:29,476 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:35:29,484 - distributed.worker - INFO -         Registered to:    tcp://172.16.3.56:38839
2023-05-10 20:35:29,484 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:35:29,485 - distributed.core - INFO - Starting established connection to tcp://172.16.3.56:38839
/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in divide
  return func(*(_execute_task(a, cache) for a in args))
2023-05-10 20:39:56,058 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 37.58 GiB -- Worker memory limit: 46.57 GiB
2023-05-10 20:40:00,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-10 20:40:02,474 - distributed.nanny.memory - WARNING - Worker tcp://172.16.3.126:36031 (pid=142207) exceeded 95% memory budget. Restarting...
2023-05-10 20:40:05,934 - distributed.nanny - INFO - Worker process 142207 was killed by signal 15
2023-05-10 20:40:06,198 - distributed.nanny - WARNING - Restarting worker
2023-05-10 20:40:16,807 - distributed.worker - INFO -       Start worker at:   tcp://172.16.3.126:44946
2023-05-10 20:40:16,808 - distributed.worker - INFO -          Listening to:   tcp://172.16.3.126:44946
2023-05-10 20:40:16,808 - distributed.worker - INFO -           Worker name:             SLURMCluster-7
2023-05-10 20:40:16,808 - distributed.worker - INFO -          dashboard at:         172.16.3.126:34332
2023-05-10 20:40:16,808 - distributed.worker - INFO - Waiting to connect to:    tcp://172.16.3.56:38839
2023-05-10 20:40:16,808 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:40:16,808 - distributed.worker - INFO -               Threads:                         36
2023-05-10 20:40:16,808 - distributed.worker - INFO -                Memory:                  46.57 GiB
2023-05-10 20:40:16,808 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4vtv9n_c
2023-05-10 20:40:16,809 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:40:16,817 - distributed.worker - INFO -         Registered to:    tcp://172.16.3.56:38839
2023-05-10 20:40:16,817 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:40:16,818 - distributed.core - INFO - Starting established connection to tcp://172.16.3.56:38839
/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in divide
  return func(*(_execute_task(a, cache) for a in args))
2023-05-10 20:40:46,343 - distributed.core - INFO - Removing comms to tcp://172.16.3.127:43757
2023-05-10 20:41:11,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-10 20:41:19,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-10 20:41:19,510 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 37.92 GiB -- Worker memory limit: 46.57 GiB
2023-05-10 20:41:31,074 - distributed.nanny.memory - WARNING - Worker tcp://172.16.3.126:44946 (pid=142898) exceeded 95% memory budget. Restarting...
2023-05-10 20:41:35,529 - distributed.nanny - INFO - Worker process 142898 was killed by signal 15
2023-05-10 20:41:36,606 - distributed.nanny - WARNING - Restarting worker
2023-05-10 20:41:43,391 - distributed.worker - INFO -       Start worker at:   tcp://172.16.3.126:39636
2023-05-10 20:41:43,405 - distributed.worker - INFO -          Listening to:   tcp://172.16.3.126:39636
2023-05-10 20:41:43,405 - distributed.worker - INFO -           Worker name:             SLURMCluster-7
2023-05-10 20:41:43,405 - distributed.worker - INFO -          dashboard at:         172.16.3.126:33641
2023-05-10 20:41:43,405 - distributed.worker - INFO - Waiting to connect to:    tcp://172.16.3.56:38839
2023-05-10 20:41:43,405 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:41:43,405 - distributed.worker - INFO -               Threads:                         36
2023-05-10 20:41:43,405 - distributed.worker - INFO -                Memory:                  46.57 GiB
2023-05-10 20:41:43,405 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g0kee2t5
2023-05-10 20:41:43,405 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:41:43,411 - distributed.worker - INFO -         Registered to:    tcp://172.16.3.56:38839
2023-05-10 20:41:43,411 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:41:43,412 - distributed.core - INFO - Starting established connection to tcp://172.16.3.56:38839
/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in divide
  return func(*(_execute_task(a, cache) for a in args))
2023-05-10 20:48:43,667 - distributed.utils_perf - INFO - full garbage collection released 1.40 GiB from 239 reference cycles (threshold: 9.54 MiB)
2023-05-10 20:48:53,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-10 20:49:07,474 - distributed.nanny.memory - WARNING - Worker tcp://172.16.3.126:39636 (pid=143444) exceeded 95% memory budget. Restarting...
2023-05-10 20:49:11,357 - distributed.nanny - INFO - Worker process 143444 was killed by signal 15
2023-05-10 20:49:11,725 - distributed.nanny - WARNING - Restarting worker
2023-05-10 20:49:18,873 - distributed.worker - INFO -       Start worker at:   tcp://172.16.3.126:37364
2023-05-10 20:49:18,874 - distributed.worker - INFO -          Listening to:   tcp://172.16.3.126:37364
2023-05-10 20:49:18,874 - distributed.worker - INFO -           Worker name:             SLURMCluster-7
2023-05-10 20:49:18,874 - distributed.worker - INFO -          dashboard at:         172.16.3.126:45499
2023-05-10 20:49:18,874 - distributed.worker - INFO - Waiting to connect to:    tcp://172.16.3.56:38839
2023-05-10 20:49:18,874 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:49:18,874 - distributed.worker - INFO -               Threads:                         36
2023-05-10 20:49:18,875 - distributed.worker - INFO -                Memory:                  46.57 GiB
2023-05-10 20:49:18,875 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-m_wh8lqk
2023-05-10 20:49:18,875 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:49:18,882 - distributed.worker - INFO -         Registered to:    tcp://172.16.3.56:38839
2023-05-10 20:49:18,882 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 20:49:18,883 - distributed.core - INFO - Starting established connection to tcp://172.16.3.56:38839
2023-05-10 20:49:20,580 - distributed.worker - ERROR - Worker stream died during communication: tcp://172.16.3.125:44648
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/core.py", line 329, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://172.16.3.126:56628 remote=tcp://172.16.3.125:44648>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2060, in gather_dep
    response = await get_data_from_worker(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 1510, in connect
    return await connect_attempt
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 1431, in _connect
    comm = await connect(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/core.py", line 334, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://172.16.3.125:44648 after 30 s
2023-05-10 20:49:20,671 - distributed.core - INFO - Removing comms to tcp://172.16.3.125:44648
2023-05-10 20:49:23,205 - distributed.core - INFO - Removing comms to tcp://172.16.3.55:45157
/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in divide
  return func(*(_execute_task(a, cache) for a in args))
2023-05-10 20:51:52,216 - distributed.utils_perf - INFO - full garbage collection released 695.30 MiB from 261 reference cycles (threshold: 9.54 MiB)
2023-05-10 20:52:30,118 - distributed.worker - ERROR - Worker stream died during communication: tcp://172.16.3.125:42594
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 235, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2060, in gather_dep
    response = await get_data_from_worker(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2895, in get_data_from_worker
    response = await send_recv(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 1028, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://172.16.3.126:42712 remote=tcp://172.16.3.125:42594>: Stream is closed
2023-05-10 20:52:30,128 - distributed.core - INFO - Removing comms to tcp://172.16.3.125:42594
2023-05-10 20:52:34,754 - distributed.worker - ERROR - Worker stream died during communication: tcp://172.16.3.55:35203
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 235, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2060, in gather_dep
    response = await get_data_from_worker(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2895, in get_data_from_worker
    response = await send_recv(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 1028, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://172.16.3.126:45655 remote=tcp://172.16.3.55:35203>: Stream is closed
2023-05-10 20:52:34,763 - distributed.core - INFO - Removing comms to tcp://172.16.3.55:35203
2023-05-10 20:52:37,172 - distributed.worker - ERROR - Worker stream died during communication: tcp://172.16.3.128:35334
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2060, in gather_dep
    response = await get_data_from_worker(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2895, in get_data_from_worker
    response = await send_recv(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 1028, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://172.16.3.126:38639 remote=tcp://172.16.3.128:35334>: Stream is closed
2023-05-10 20:52:37,184 - distributed.core - INFO - Removing comms to tcp://172.16.3.128:35334
2023-05-10 20:54:39,000 - distributed.utils_perf - INFO - full garbage collection released 0.95 GiB from 169 reference cycles (threshold: 9.54 MiB)
2023-05-10 20:55:13,746 - distributed.worker - ERROR - Worker stream died during communication: tcp://172.16.3.125:41877
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 235, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2060, in gather_dep
    response = await get_data_from_worker(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2895, in get_data_from_worker
    response = await send_recv(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 1028, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://172.16.3.126:53251 remote=tcp://172.16.3.125:41877>: Stream is closed
2023-05-10 20:55:13,759 - distributed.core - INFO - Removing comms to tcp://172.16.3.125:41877
2023-05-10 20:55:14,335 - distributed.worker - ERROR - Worker stream died during communication: tcp://172.16.3.127:33542
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 235, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2060, in gather_dep
    response = await get_data_from_worker(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2895, in get_data_from_worker
    response = await send_recv(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 1028, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://172.16.3.126:51041 remote=tcp://172.16.3.127:33542>: Stream is closed
2023-05-10 20:55:14,343 - distributed.core - INFO - Removing comms to tcp://172.16.3.127:33542
2023-05-10 20:55:15,678 - distributed.worker - ERROR - Worker stream died during communication: tcp://172.16.3.55:45988
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 235, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2060, in gather_dep
    response = await get_data_from_worker(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2895, in get_data_from_worker
    response = await send_recv(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 1028, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://172.16.3.126:44237 remote=tcp://172.16.3.55:45988>: Stream is closed
2023-05-10 20:55:15,691 - distributed.core - INFO - Removing comms to tcp://172.16.3.55:45988
2023-05-10 20:55:18,079 - distributed.worker - ERROR - Worker stream died during communication: tcp://172.16.3.128:41727
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 235, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2060, in gather_dep
    response = await get_data_from_worker(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2895, in get_data_from_worker
    response = await send_recv(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 1028, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://172.16.3.126:51194 remote=tcp://172.16.3.128:41727>: Stream is closed
2023-05-10 20:55:18,086 - distributed.core - INFO - Removing comms to tcp://172.16.3.128:41727
2023-05-10 20:56:21,555 - distributed.worker - ERROR - Encountered invalid state
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 219, in wrapper
    return method(self, *args, **kwargs)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 1941, in handle_stimulus
    super().handle_stimulus(*stims)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker_state_machine.py", line 3706, in handle_stimulus
    instructions = self.state.handle_stimulus(*stims)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker_state_machine.py", line 1355, in handle_stimulus
    recs, instr = self._handle_event(stim)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/functools.py", line 926, in _method
    return method.__get__(obj, cls)(*args, **kwargs)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker_state_machine.py", line 2794, in _handle_remove_replicas
    raise RuntimeError("Encountered invalid state")  # pragma: no cover
RuntimeError: Encountered invalid state
2023-05-10 20:56:21,556 - distributed.worker - INFO - Stopping worker at tcp://172.16.3.126:37364. Reason: worker-handle-scheduler-connection-broken
2023-05-10 20:56:21,557 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.16.3.126:34628 remote=tcp://172.16.3.56:38839>
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-05-10 20:56:21,598 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.16.3.126:45897'. Reason: worker-handle-scheduler-connection-broken
2023-05-10 20:56:24,991 - distributed.nanny - INFO - Worker closed
2023-05-10 20:56:24,991 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x2aaab75305b0>>, <Task finished name='Task-7' coro=<Worker.handle_scheduler() done, defined at /vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py:203> exception=RuntimeError('Encountered invalid state')>)
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 1283, in handle_scheduler
    await self.handle_stream(comm)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 916, in handle_stream
    handler(**merge(extra, msg))
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 1926, in _
    self.handle_stimulus(event)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 219, in wrapper
    return method(self, *args, **kwargs)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 1941, in handle_stimulus
    super().handle_stimulus(*stims)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker_state_machine.py", line 3706, in handle_stimulus
    instructions = self.state.handle_stimulus(*stims)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker_state_machine.py", line 1355, in handle_stimulus
    recs, instr = self._handle_event(stim)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/functools.py", line 926, in _method
    return method.__get__(obj, cls)(*args, **kwargs)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker_state_machine.py", line 2794, in _handle_remove_replicas
    raise RuntimeError("Encountered invalid state")  # pragma: no cover
RuntimeError: Encountered invalid state
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-7' coro=<Worker.handle_scheduler() done, defined at /vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py:203> exception=RuntimeError('Encountered invalid state')>
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 1283, in handle_scheduler
    await self.handle_stream(comm)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 916, in handle_stream
    handler(**merge(extra, msg))
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 1926, in _
    self.handle_stimulus(event)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 219, in wrapper
    return method(self, *args, **kwargs)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 1941, in handle_stimulus
    super().handle_stimulus(*stims)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker_state_machine.py", line 3706, in handle_stimulus
    instructions = self.state.handle_stimulus(*stims)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker_state_machine.py", line 1355, in handle_stimulus
    recs, instr = self._handle_event(stim)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/functools.py", line 926, in _method
    return method.__get__(obj, cls)(*args, **kwargs)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker_state_machine.py", line 2794, in _handle_remove_replicas
    raise RuntimeError("Encountered invalid state")  # pragma: no cover
RuntimeError: Encountered invalid state
2023-05-10 20:56:31,783 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.16.3.126:45897'. Reason: nanny-close-gracefully
2023-05-10 20:56:31,784 - distributed.dask_worker - INFO - End worker
