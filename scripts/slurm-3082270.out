2023-05-03 10:46:41,029 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.16.3.63:42769'
2023-05-03 10:46:47,954 - distributed.worker - INFO -       Start worker at:    tcp://172.16.3.63:35620
2023-05-03 10:46:47,954 - distributed.worker - INFO -          Listening to:    tcp://172.16.3.63:35620
2023-05-03 10:46:47,954 - distributed.worker - INFO -           Worker name:            SLURMCluster-15
2023-05-03 10:46:47,954 - distributed.worker - INFO -          dashboard at:          172.16.3.63:36682
2023-05-03 10:46:47,954 - distributed.worker - INFO - Waiting to connect to:   tcp://172.16.3.158:35777
2023-05-03 10:46:47,954 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 10:46:47,954 - distributed.worker - INFO -               Threads:                         36
2023-05-03 10:46:47,954 - distributed.worker - INFO -                Memory:                 178.81 GiB
2023-05-03 10:46:47,954 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-d4aakyox
2023-05-03 10:46:47,955 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 10:46:47,960 - distributed.worker - INFO -         Registered to:   tcp://172.16.3.158:35777
2023-05-03 10:46:47,960 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 10:46:47,960 - distributed.core - INFO - Starting established connection to tcp://172.16.3.158:35777
2023-05-03 10:48:19,032 - distributed.utils_perf - INFO - full garbage collection released 1.31 GiB from 380 reference cycles (threshold: 9.54 MiB)
2023-05-03 10:48:43,399 - distributed.utils_perf - INFO - full garbage collection released 435.64 MiB from 439 reference cycles (threshold: 9.54 MiB)
2023-05-03 10:48:49,077 - distributed.utils_perf - INFO - full garbage collection released 733.47 MiB from 520 reference cycles (threshold: 9.54 MiB)
2023-05-03 10:48:51,836 - distributed.utils_perf - INFO - full garbage collection released 1.71 GiB from 779 reference cycles (threshold: 9.54 MiB)
2023-05-03 10:48:54,839 - distributed.utils_perf - INFO - full garbage collection released 1.12 GiB from 539 reference cycles (threshold: 9.54 MiB)
2023-05-03 10:49:01,062 - distributed.utils_perf - INFO - full garbage collection released 5.49 GiB from 339 reference cycles (threshold: 9.54 MiB)
2023-05-03 10:49:08,889 - distributed.utils_perf - INFO - full garbage collection released 1.81 GiB from 419 reference cycles (threshold: 9.54 MiB)
2023-05-03 10:49:11,931 - distributed.utils_perf - INFO - full garbage collection released 69.46 MiB from 339 reference cycles (threshold: 9.54 MiB)
2023-05-03 10:49:22,669 - distributed.utils_perf - INFO - full garbage collection released 34.50 MiB from 519 reference cycles (threshold: 9.54 MiB)
slurmstepd: error: *** JOB 3082270 ON pn013 CANCELLED AT 2023-05-03T11:05:40 ***
