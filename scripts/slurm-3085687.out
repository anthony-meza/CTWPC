2023-05-10 10:17:20,118 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.16.3.128:40733'
2023-05-10 10:17:20,546 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-evqdnn9s', purging
2023-05-10 10:17:25,658 - distributed.worker - INFO -       Start worker at:   tcp://172.16.3.128:41087
2023-05-10 10:17:25,658 - distributed.worker - INFO -          Listening to:   tcp://172.16.3.128:41087
2023-05-10 10:17:25,659 - distributed.worker - INFO -           Worker name:             SLURMCluster-1
2023-05-10 10:17:25,659 - distributed.worker - INFO -          dashboard at:         172.16.3.128:34180
2023-05-10 10:17:25,659 - distributed.worker - INFO - Waiting to connect to:    tcp://172.16.3.56:40509
2023-05-10 10:17:25,659 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 10:17:25,659 - distributed.worker - INFO -               Threads:                         30
2023-05-10 10:17:25,659 - distributed.worker - INFO -                Memory:                 139.70 GiB
2023-05-10 10:17:25,659 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4eh2h7mn
2023-05-10 10:17:25,659 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 10:17:25,933 - distributed.worker - INFO -         Registered to:    tcp://172.16.3.56:40509
2023-05-10 10:17:25,933 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 10:17:25,934 - distributed.core - INFO - Starting established connection to tcp://172.16.3.56:40509
2023-05-10 10:17:28,845 - distributed.worker - WARNING - Compute Failed
Key:       open_dataset-bafc86fb-9346-4799-a285-5ac08b373275
Function:  execute_task
args:      ((<function apply at 0x2aaab31b9750>, <function open_dataset at 0x2aaacbd27910>, ['/vortexfs1/home/anthony.meza/Atmospheric Rivers and Waves/ERA5_data/ERA5_1998.nc'], (<class 'dict'>, [['engine', None], ['chunks', (<class 'dict'>, [['latitude', -1], ['longitude', -1], ['time', -1]])]])))
kwargs:    {}
Exception: "RuntimeError('NetCDF: Not a valid ID')"

Exception ignored in: <function CachingFileManager.__del__ at 0x2aaacbcf1750>
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 249, in __del__
    self.close(needs_lock=False)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 233, in close
    file.close()
  File "src/netCDF4/_netCDF4.pyx", line 2607, in netCDF4._netCDF4.Dataset.close
  File "src/netCDF4/_netCDF4.pyx", line 2570, in netCDF4._netCDF4.Dataset._close
  File "src/netCDF4/_netCDF4.pyx", line 2014, in netCDF4._netCDF4._ensure_nc_success
RuntimeError: NetCDF: Not a valid ID
Exception ignored in: <function CachingFileManager.__del__ at 0x2aaacbcf1750>
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 249, in __del__
    self.close(needs_lock=False)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 233, in close
    file.close()
  File "src/netCDF4/_netCDF4.pyx", line 2607, in netCDF4._netCDF4.Dataset.close
  File "src/netCDF4/_netCDF4.pyx", line 2570, in netCDF4._netCDF4.Dataset._close
  File "src/netCDF4/_netCDF4.pyx", line 2014, in netCDF4._netCDF4._ensure_nc_success
RuntimeError: NetCDF: Not a valid ID
Exception ignored in: <function CachingFileManager.__del__ at 0x2aaacbcf1750>
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 249, in __del__
    self.close(needs_lock=False)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 233, in close
    file.close()
  File "src/netCDF4/_netCDF4.pyx", line 2607, in netCDF4._netCDF4.Dataset.close
  File "src/netCDF4/_netCDF4.pyx", line 2570, in netCDF4._netCDF4.Dataset._close
  File "src/netCDF4/_netCDF4.pyx", line 2014, in netCDF4._netCDF4._ensure_nc_success
RuntimeError: NetCDF: Not a valid ID
2023-05-10 10:19:00,591 - distributed.nanny - INFO - Worker process 175666 was killed by signal 11
2023-05-10 10:19:00,748 - distributed.nanny - WARNING - Restarting worker
2023-05-10 10:19:05,751 - distributed.worker - INFO -       Start worker at:   tcp://172.16.3.128:40688
2023-05-10 10:19:05,751 - distributed.worker - INFO -          Listening to:   tcp://172.16.3.128:40688
2023-05-10 10:19:05,751 - distributed.worker - INFO -           Worker name:             SLURMCluster-1
2023-05-10 10:19:05,751 - distributed.worker - INFO -          dashboard at:         172.16.3.128:35600
2023-05-10 10:19:05,751 - distributed.worker - INFO - Waiting to connect to:    tcp://172.16.3.56:40509
2023-05-10 10:19:05,751 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 10:19:05,751 - distributed.worker - INFO -               Threads:                         30
2023-05-10 10:19:05,751 - distributed.worker - INFO -                Memory:                 139.70 GiB
2023-05-10 10:19:05,751 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nvrab2ry
2023-05-10 10:19:05,751 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 10:19:05,757 - distributed.worker - INFO -         Registered to:    tcp://172.16.3.56:40509
2023-05-10 10:19:05,757 - distributed.worker - INFO - -------------------------------------------------
2023-05-10 10:19:05,758 - distributed.core - INFO - Starting established connection to tcp://172.16.3.56:40509
2023-05-10 10:19:13,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-10 10:19:23,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-10 10:19:30,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
slurmstepd: error: *** JOB 3085687 ON pn078 CANCELLED AT 2023-05-10T12:17:33 DUE TO TIME LIMIT ***
