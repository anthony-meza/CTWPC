2023-05-04 15:19:18,163 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.16.3.82:43287'
2023-05-04 15:19:27,281 - distributed.worker - INFO -       Start worker at:    tcp://172.16.3.82:33831
2023-05-04 15:19:27,281 - distributed.worker - INFO -          Listening to:    tcp://172.16.3.82:33831
2023-05-04 15:19:27,281 - distributed.worker - INFO -           Worker name:            SLURMCluster-12
2023-05-04 15:19:27,281 - distributed.worker - INFO -          dashboard at:          172.16.3.82:43786
2023-05-04 15:19:27,281 - distributed.worker - INFO - Waiting to connect to:   tcp://172.16.3.106:43862
2023-05-04 15:19:27,281 - distributed.worker - INFO - -------------------------------------------------
2023-05-04 15:19:27,281 - distributed.worker - INFO -               Threads:                         36
2023-05-04 15:19:27,281 - distributed.worker - INFO -                Memory:                 178.81 GiB
2023-05-04 15:19:27,282 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-den6iefc
2023-05-04 15:19:27,282 - distributed.worker - INFO - -------------------------------------------------
2023-05-04 15:19:27,288 - distributed.worker - INFO -         Registered to:   tcp://172.16.3.106:43862
2023-05-04 15:19:27,288 - distributed.worker - INFO - -------------------------------------------------
2023-05-04 15:19:27,288 - distributed.core - INFO - Starting established connection to tcp://172.16.3.106:43862
2023-05-04 15:24:44,861 - distributed.core - INFO - Removing comms to tcp://172.16.3.185:41432
2023-05-04 16:25:43,287 - distributed.core - INFO - Removing comms to tcp://172.16.3.65:37409
2023-05-04 16:25:43,294 - distributed.core - INFO - Removing comms to tcp://172.16.3.67:44799
2023-05-04 16:25:43,294 - distributed.core - INFO - Removing comms to tcp://172.16.3.63:33229
2023-05-04 16:25:43,301 - distributed.core - INFO - Removing comms to tcp://172.16.3.68:32915
2023-05-04 16:25:43,332 - distributed.core - INFO - Removing comms to tcp://172.16.3.64:40969
2023-05-04 16:25:43,344 - distributed.core - INFO - Removing comms to tcp://172.16.3.70:38567
2023-05-04 16:25:43,350 - distributed.core - INFO - Removing comms to tcp://172.16.3.69:44899
2023-05-04 16:43:19,278 - distributed.utils_perf - INFO - full garbage collection released 4.27 GiB from 741 reference cycles (threshold: 9.54 MiB)
2023-05-04 16:43:59,566 - distributed.utils_perf - INFO - full garbage collection released 3.17 GiB from 619 reference cycles (threshold: 9.54 MiB)
2023-05-04 16:44:13,470 - distributed.utils_perf - INFO - full garbage collection released 2.62 GiB from 639 reference cycles (threshold: 9.54 MiB)
2023-05-04 16:44:39,693 - distributed.utils_perf - INFO - full garbage collection released 1.47 GiB from 699 reference cycles (threshold: 9.54 MiB)
2023-05-04 16:45:34,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-04 16:45:42,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-04 16:45:52,609 - distributed.utils_perf - INFO - full garbage collection released 22.79 MiB from 719 reference cycles (threshold: 9.54 MiB)
2023-05-04 16:45:52,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-04 16:46:02,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-04 16:46:13,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-04 16:46:25,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-04 16:46:36,541 - distributed.worker - ERROR - failed during get data with tcp://172.16.3.82:33831 -> tcp://172.16.3.83:42714
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
TimeoutError: [Errno 110] Connection timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 1787, in get_data
    response = await comm.read(deserializers=serializers)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://172.16.3.82:33831 remote=tcp://172.16.3.83:38833>: TimeoutError: [Errno 110] Connection timed out
2023-05-04 16:46:36,543 - distributed.core - INFO - Lost connection to 'tcp://172.16.3.83:38833'
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
TimeoutError: [Errno 110] Connection timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 832, in _handle_comm
    result = await result
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 991, in wrapper
    return await func(self, *args, **kwargs)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 1787, in get_data
    response = await comm.read(deserializers=serializers)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://172.16.3.82:33831 remote=tcp://172.16.3.83:38833>: TimeoutError: [Errno 110] Connection timed out
2023-05-04 16:46:36,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-04 16:46:37,851 - distributed.core - INFO - Removing comms to tcp://172.16.3.83:42714
2023-05-04 16:46:38,048 - distributed.worker - ERROR - Worker stream died during communication: tcp://172.16.3.83:42714
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 235, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2060, in gather_dep
    response = await get_data_from_worker(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 2895, in get_data_from_worker
    response = await send_recv(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 1028, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://172.16.3.82:33375 remote=tcp://172.16.3.83:42714>: Stream is closed
2023-05-04 16:46:47,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-04 16:47:02,530 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)
2023-05-04 16:47:02,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-04 16:47:12,926 - distributed.utils_perf - WARNING - full garbage collections took 28% CPU time recently (threshold: 10%)
2023-05-04 16:47:13,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-04 16:47:22,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-04 16:47:31,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-05-04 16:47:33,472 - distributed.utils_perf - WARNING - full garbage collections took 34% CPU time recently (threshold: 10%)
2023-05-04 16:47:37,623 - distributed.utils_perf - WARNING - full garbage collections took 34% CPU time recently (threshold: 10%)
2023-05-04 16:47:43,182 - distributed.utils_perf - WARNING - full garbage collections took 34% CPU time recently (threshold: 10%)
2023-05-04 16:47:46,928 - distributed.utils_perf - WARNING - full garbage collections took 34% CPU time recently (threshold: 10%)
2023-05-04 16:47:51,336 - distributed.utils_perf - WARNING - full garbage collections took 34% CPU time recently (threshold: 10%)
2023-05-04 16:48:34,378 - distributed.utils_perf - WARNING - full garbage collections took 33% CPU time recently (threshold: 10%)
2023-05-04 16:48:34,382 - distributed.utils_perf - INFO - full garbage collection released 12.87 MiB from 260 reference cycles (threshold: 9.54 MiB)
2023-05-04 16:49:25,328 - distributed.utils_perf - WARNING - full garbage collections took 31% CPU time recently (threshold: 10%)
2023-05-04 16:49:32,084 - distributed.worker - ERROR - failed during get data with tcp://172.16.3.82:33831 -> tcp://172.16.3.189:34085
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
TimeoutError: [Errno 110] Connection timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 1787, in get_data
    response = await comm.read(deserializers=serializers)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://172.16.3.82:33831 remote=tcp://172.16.3.189:53808>: TimeoutError: [Errno 110] Connection timed out
2023-05-04 16:49:32,085 - distributed.core - INFO - Lost connection to 'tcp://172.16.3.189:53808'
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
TimeoutError: [Errno 110] Connection timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 832, in _handle_comm
    result = await result
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 991, in wrapper
    return await func(self, *args, **kwargs)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 1787, in get_data
    response = await comm.read(deserializers=serializers)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://172.16.3.82:33831 remote=tcp://172.16.3.189:53808>: TimeoutError: [Errno 110] Connection timed out
2023-05-04 16:49:51,445 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)
2023-05-04 16:49:59,849 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)
2023-05-04 16:50:03,642 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)
2023-05-04 16:50:11,581 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)
2023-05-04 16:50:46,808 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)
2023-05-04 16:50:59,463 - distributed.utils_perf - WARNING - full garbage collections took 28% CPU time recently (threshold: 10%)
2023-05-04 16:51:30,186 - distributed.utils_perf - WARNING - full garbage collections took 28% CPU time recently (threshold: 10%)
2023-05-04 16:51:38,549 - distributed.utils_perf - WARNING - full garbage collections took 27% CPU time recently (threshold: 10%)
2023-05-04 16:51:49,841 - distributed.utils_perf - WARNING - full garbage collections took 27% CPU time recently (threshold: 10%)
2023-05-04 16:51:51,296 - distributed.utils_perf - WARNING - full garbage collections took 27% CPU time recently (threshold: 10%)
2023-05-04 16:52:04,233 - distributed.utils_perf - WARNING - full garbage collections took 26% CPU time recently (threshold: 10%)
2023-05-04 16:52:05,635 - distributed.utils_perf - WARNING - full garbage collections took 26% CPU time recently (threshold: 10%)
2023-05-04 16:52:19,097 - distributed.utils_perf - WARNING - full garbage collections took 27% CPU time recently (threshold: 10%)
2023-05-04 16:52:52,606 - distributed.utils_perf - WARNING - full garbage collections took 27% CPU time recently (threshold: 10%)
2023-05-04 16:53:08,164 - distributed.utils_perf - WARNING - full garbage collections took 25% CPU time recently (threshold: 10%)
2023-05-04 16:53:09,970 - distributed.utils_perf - WARNING - full garbage collections took 25% CPU time recently (threshold: 10%)
2023-05-04 16:53:20,557 - distributed.utils_perf - WARNING - full garbage collections took 24% CPU time recently (threshold: 10%)
2023-05-04 16:53:21,909 - distributed.utils_perf - WARNING - full garbage collections took 24% CPU time recently (threshold: 10%)
2023-05-04 16:53:47,748 - distributed.utils_perf - WARNING - full garbage collections took 23% CPU time recently (threshold: 10%)
2023-05-04 16:54:06,645 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 1237, in heartbeat
    response = await retry_operation(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 1269, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/core.py", line 1028, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.16.3.82:48112 remote=tcp://172.16.3.106:43862>: ConnectionResetError: [Errno 104] Connection reset by peer
2023-05-04 16:54:06,648 - distributed.core - INFO - Connection to tcp://172.16.3.106:43862 has been closed.
2023-05-04 16:54:06,648 - distributed.worker - INFO - Stopping worker at tcp://172.16.3.82:33831. Reason: worker-handle-scheduler-connection-broken
2023-05-04 16:54:06,656 - distributed.worker - ERROR - failed during get data with tcp://172.16.3.82:33831 -> tcp://172.16.3.189:34085
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 1787, in get_data
    response = await comm.read(deserializers=serializers)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://172.16.3.82:33831 remote=tcp://172.16.3.189:53966>: Stream is closed
2023-05-04 16:54:06,667 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.16.3.82:43287'. Reason: worker-handle-scheduler-connection-broken
2023-05-04 16:54:08,510 - distributed.worker - ERROR - failed during get data with tcp://172.16.3.82:33831 -> tcp://172.16.3.104:39749
Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/worker.py", line 1787, in get_data
    response = await comm.read(deserializers=serializers)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://172.16.3.82:33831 remote=tcp://172.16.3.104:52878>: ConnectionResetError: [Errno 104] Connection reset by peer
2023-05-04 16:54:08,774 - distributed.client - ERROR - 
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 511, in connect
    convert_stream_closed_error(self, e)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x2abf778749d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/client.py", line 1323, in _reconnect
    await self._ensure_connected(timeout=timeout)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/client.py", line 1353, in _ensure_connected
    comm = await connect(
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/site-packages/distributed/comm/core.py", line 316, in connect
    await asyncio.sleep(backoff)
  File "/vortexfs1/home/anthony.meza/mambaforge/envs/atm_rivers/lib/python3.10/asyncio/tasks.py", line 605, in sleep
    return await future
asyncio.exceptions.CancelledError
2023-05-04 16:54:08,789 - distributed.nanny - INFO - Worker closed
2023-05-04 16:54:10,794 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-05-04 16:54:17,893 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.16.3.82:43287'. Reason: nanny-close-gracefully
2023-05-04 16:54:17,895 - distributed.dask_worker - INFO - End worker
